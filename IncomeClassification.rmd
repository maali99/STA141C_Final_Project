---
title: "STA 141C Final Project"
author: "Sukhween Bhullar"
date: "3/17/2020"
output: html_document
---

```{r}
#read the data and rename the columns
library(readr)
library(ggplot2)
library(tidyverse)
library(readr)
url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
data1 <- read.csv(url1, header = FALSE)

colnames(data1) <- c("age",
                     "workclass",
                     "fnlwgt",
                     "education",
                     "education_num",
                     "marital_status",
                     "occupation",
                     "relationship",
                     "race",
                     "sex",
                     "capital_gain",
                     "capital_loss",
                     "hours_per_week",
                     "native_country",
                     "income")

#take out the rows with the missing data
data1[data1 == " ?"] = NA
data1 = na.omit(data1)
```

Some Pre-liminary Data Exploration
```{r}

attach(data1)  

#age + race + sex + education_num + hours_per_week

# Plot ratios of income in male group and female group
fig = ggplot(data = data1, mapping = aes(x = sex, fill = income))
fig1 = fig + geom_bar(position = "fill")
fig1

# plot the number of income > 50K and income <=50K grouped by sex and education_num
fig = ggplot(data = data1, mapping = aes(x = education_num, fill = income))
fig2 = fig + geom_bar() + facet_grid(sex, scales = "free_y")
fig2

# plot  the number of income > 50K and income <=50K grouped by race and education
fig = ggplot(data = data1, mapping = aes(x = education_num, fill = income))
fig3 = fig + geom_bar() + facet_grid(race, scales = "free_y")
fig3

# plot the distribution of age groupped by income
fig = ggplot(data = data1, mapping = aes(x = age, fill = income, color = income))
fig4 = fig + geom_density(alpha = 0.3)
fig4

# plot the percentage of income >= 50K in each hours_per_week group
# hours_income = table(data1[,c('hours_per_week','income')])
# hours_income <- as.data.frame(prop.table(hours_income, 1))
# hours_income_over50 = hours_income[(nrow(hours_income)/2+1):nrow(hours_income),]
# hours_income_over50

fig = ggplot(data = hours_income_over50, 
             mapping = aes(
             x = hours_per_week, 
             y = Freq,
             group = income))
fig4 = fig + geom_area(fill = "chartreuse4",alpha = 0.7) + geom_smooth()+
  scale_x_discrete(
    breaks = c(1, 20, 40, 60, 80, 99)
  )+
  labs(
    y = "Percentage of Income >= 50K"
  )
fig4


detach(data1)

#histograms of the continuous quanitative variables 
par(mfrow=c(3,2))
hist(data$age, xlab="Age", main="Histogram of Age")
hist(data$capital_gain, xlab="Capital Gain", main="Histogram of Capital Gain")
hist(data$capital_loss, xlab="Capital Loss", main="Histogram of Capital Loss")
hist(data$education_num, xlab="Education Number", main="Histogram of Education Number")
hist(data$hours_per_week, xlab="Hours Per Week", main="Histogram of Hours Per Week")
#hist(data_vars$fnlwgt, xlab="fnlwgt")
par(mfrow=c(1,1))

#correlation matrix with each qualitative variable and the response, based on a chisquared test
attach(data)
library(corrplot)
par(mfrow=c(1,3))
workclass_income<-chisq.test(table(droplevels(workclass), 
      income_group))
corrplot(workclass_income$residuals, is.cor=FALSE) 
occupation_income<-chisq.test(table(droplevels(occupation), 
      income_group))
corrplot(occupation_income$residuals, is.cor=FALSE) 
education_income<-chisq.test(table(education, 
      income_group))
corrplot(education_income$residuals, is.cor=FALSE)
race_income<-chisq.test(table(race, 
      income_group))
corrplot(race_income$residuals, is.cor=FALSE)
sex_income<-chisq.test(table(sex, 
      income_group))
corrplot(sex_income$residuals, is.cor=FALSE)
marital_income<-chisq.test(table(marital_status, 
      income_group))
corrplot(marital_income$residuals, is.cor=FALSE)

cor(age, education_num)
cor(age,hours_per_week)
cor(education_num, hours_per_week)
par(mfrow=c(1,1))
```

```{r}
#split the rows into testing data and training data 

data1_test= data1[24131:nrow(data1),] #testing data
data1= data1[1:24130,] #training data

```

Logistic Regression (Without Bootstrapping)
```{r}
fullmodel <- glm(income ~., family = binomial, data = data1)
summary(fullmodel)
```

```{r}
#the full model gives us approximately 90 estimates, so we shifted to a simpler model, taking out variables
#with a lot of levels
model1 <- glm(income ~ age + race + sex + education_num + hours_per_week, family = binomial, data = data1)

error1 <- summary(model1)$coef[,1]
error2 <- model1$coefficients
```

Implementing Bag of Little Bootstraps with Logistic Regression on `model1`
```{r}
# For every iteration
# randomize data
# find the slopes

suppressMessages(library(tidyverse))
library(furrr)

plan(multiprocess, workers = 4)
B = 100
n = nrow(data1)

m <- 10
groups <- sample(seq_len(m), n, replace = TRUE)
map(seq_len(m), ~write_csv(data1[groups==.,], str_c("part",.,".csv")))

single_boot <- function(i, ind_part){
  subsample = read_csv(str_c("part",ind_part, ".csv"))
  freqs = rmultinom(1, n, rep(1, nrow(subsample)))
  model1 <- glm(as.factor(income) ~ age + race + sex + education_num + hours_per_week, weight = freqs, family = binomial, data = subsample)
  as.data.frame(t(as.matrix(model1$coefficients)))
}

```

After getting running the bootstraps we implemented three different ways to get an estimate of the slopes by using: 
+confidence interval
+median
+mean
```{r}
#Confidence interval at alpha =.05
ci_list = future_map(seq_len(m), ~{
  map_dfr(seq_len(B), single_boot, ind_part =.) %>%
    map_dfr(~quantile(., c(0.025,0.975))) } )

ci1 <- reduce(ci_list, `+`) / length(ci_list)
```

```{r}
#median using the median function
med_list = future_map(seq_len(m), ~{
  map_dfr(seq_len(B), single_boot, ind_part =.) %>%
    map_dfr(~median(.)) } )

med1 <- reduce(med_list, `+`) / length(med_list)
```

```{r}
#mean using the mean function
mean_list = future_map(seq_len(m), ~{
  map_dfr(seq_len(B), single_boot, ind_part =.) %>%
    map_dfr(~mean(.)) } )

mean1 <- reduce(mean_list, `+`) / length(mean_list)
```

To check the accuracy of our models and to find which method (confidence interval, median and mean) gives the best result we built our own predict function to test the above models on our test data. 
```{r}
#initialize counters
incorrect_50plus=0 #counter for number of incorrect classifications of incomes greater than 50k
correct_50plus=0 #counter for number of correct classifications of incomes greater than 50k
incorrect_50less=0 #counter for number of incorrect classifications of incomes less than 50k
correct_50less=0 #counter for number of correct classifications of incomes less than 50k

predict_blb_glm <-function(method, test_data){
  
  #the accuracy vector will contain a vector of all the counters
  acccuracy_vector <- c("Incorrect(>50k)", "Correct(>50k)","Incorrect(<=50k)", "Correct(<=50k)")
  
  #from the test data select the variables of interest
  test_data <- test_data %>% 
    select(age, race, sex, education_num, hours_per_week,income)
  
  #this for loop will predict the income brackets
  for(i in seq_len(nrow(test_data))){
    
    test_val=test_data[i,]
    
    #we must use multiple if else statements to check for the race variable
    #because it's a categorical variable
    #use the odds= Intercept_est+
        #age_beta1*age +
        #`raceAsian-Pac-Islander`_beta *(0 or 1) +
        #raceBlack_beta *(0 or 1) +
        #raceOther_beta *(0 or 1) +
        #raceWhite_beta *( 0 or 1) +
        #education_num_beta *(education_num) +
        #hours_per_week_beta *(hours_per_week)
    
    if(as.character(droplevels(test_val$race))== " White"){

     odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*0 +
        method$raceBlack*0 +
        method$raceOther*0 +
        method$raceWhite*1 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }else if(as.character(droplevels(test_val$race))==" Asian-Pac-Islander"){
      odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*1 +
        method$raceBlack*0 +
        method$raceOther*0 +
        method$raceWhite*0 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }else if(as.character(droplevels(test_val$race))==" Black"){
      
      odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*0 +
        method$raceBlack*1 +
        method$raceOther*0 +
        method$raceWhite*0 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }else if(as.character(droplevels(test_val$race))==" Other"){
      
      odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*0 +
        method$raceBlack*0 +
        method$raceOther*1 +
        method$raceWhite*0 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }else if(as.character(droplevels(test_val$race))==" Amer-Indian-Eskimo"){
      
      odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*0 +
        method$raceBlack*0 +
        method$raceOther*0 +
        method$raceWhite*0 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }
    
    #use the equation exp(odds)/(1+exp(odds)) to get a probability value
    probability = exp(odds)/(1+exp(odds))
    
    # if the probability is greater than .5 then the predicted income is less than 50k 
    if(probability>.5){
      
      pred_income = " <=50K"
      #if the predicted income for the test data is equal to the actual income, increment the correct counter
      if(pred_income == as.character(droplevels(test_val$income))){ 
        
        correct_50less = correct_50less +1
      }else{
      #otherwise increment incorrect counter
        incorrect_50less = incorrect_50less +1
      }
    
    #otherwise the predicted income is greater than 50k  
    }else{
      
      pred_income = " >50K"
      #if the predicted income for the test data is equal to the actual income, increment the correct counter
      if(pred_income == as.character(droplevels(test_val$income))){
        correct_50plus = correct_50plus +1
      }else{
        #otherwise increment incorrect counter
        incorrect_50plus = incorrect_50plus +1
      }
      
    }
    
    
    accuracy_vector<- c( correct_50plus, incorrect_50plus, correct_50less, incorrect_50less)
    
  }
  
  return(accuracy_vector)
}
```

```{r}
#use mean1 to calculate an accuracy vector on the test data
accuracy_mean<- predict_blb_glm(mean1, data1_test)

mean_rate<- (accuracy_mean[1]+accuracy_mean[3])/(nrow(data1_test))*100

#use med1 to calculate an accuracy vector on the test data
accuracy_median<- predict_blb_glm(med1, data1_test)

median_rate<- (accuracy_median[1]+accuracy_median[3])/(nrow(data1_test))*100

#use the confidence interval to calculate an accuracy vector on the test data
accuracy_ci1<- predict_blb_glm(ci1[1,],data1_test) #this calculates for the lower bound
accuracy_ci2<- predict_blb_glm(ci1[2,],data1_test) #this calculates for the upper bound

ci1_rate<- (accuracy_ci1[1]+accuracy_ci1[3])/(nrow(data1_test))*100

ci2_rate<- (accuracy_ci2[1]+accuracy_ci2[3])/(nrow(data1_test))*100
```

After using calculating the accuracy rates, we can conclude that teh Logistic Regression Model is not a good choice for this data, so we switched over to using Random Forest because of how categorical our data is. 
First we will use the tree function to construct a classification and regression tree. 
```{r}
#implement the tree function to 
library(tree)
#note we cannot use all the variables because the tree function has a limit to how many levels we can have
#if we add all the variables then we would exceed that limit, so we used most of the variables, but took out a
#a few of them
tree_income <- tree(income ~ 
                      age+
                      workclass+
                      education+
                      marital_status+
                      occupation+
                      relationship+
                      race+
                      capital_gain +
                      capital_loss+
                      hours_per_week+
                      sex, data1)

plot(tree_income, type = "uniform")
text(tree_income, pretty = 1, all = TRUE, cex = 0.7)

rfPredict_income<-as.data.frame(predict(tree_income, data1_test))

incorrect_50plus2=0 #counter for number of incorrect classifications of incomes greater than 50k
correct_50plus2=0 #counter for number of correct classifications of incomes greater than 50k
incorrect_50less2=0 #counter for number of incorrect classifications of incomes less than 50k
correct_50less2=0 

calc_accuracy_rf<-function(test_data, predicted){

  for(i in seq_len(nrow(predicted))){ #if the probability is higher for >50K then it means that predicted income is >50K
    if(predicted[i,1]<predicted[i,2]){
      
      pred_income = " >50K"
    #if the predicted income is equal to the actual income at that level then increment correct
      if(pred_income == as.character(droplevels(test_data[i,]$income))){
        correct_50plus2= correct_50plus2 +1
      }
      else{
    #otherwise increment incorrect
        incorrect_50plus2 = incorrect_50plus2 + 1
      }
    }else{ #if the probability is less for >50k then it means the predicted is <=50K
      
      pred_income = " <=50K"
    #if the predicted is equal to the actual value of test data at that row 
      if(pred_income == as.character(droplevels(test_data[i,]$income))){
    #then increment the correct counter
        correct_50less2= correct_50less2+1
      }else{
    #otherwise increment incorrect
        incorrect_50less2 = incorrect_50less2 +1
      }
    }
  }
  return(c(correct_50plus2, incorrect_50plus2, correct_50less2, incorrect_50less2))
}


accuracy_val<-calc_accuracy_rf(data1_test,rfPredict_income)

(accuracy_val[1]+accuracy_val[3])/(nrow(data1_test))*100
```

After running the tree function to create a classification and regression tree, we predicted an accuracy rate to see how accurately the tree classifies the test data. Without running bootstrap, it's evident that the accuracy rate is significantly higer than the logistic regression. Now we wil use this function to create a Bag of Little Random Forests. 
```{r}
#use the library furrr
suppressMessages(library(tidyverse))
library(furrr)

plan(multiprocess, workers = 4)

#set B to 1000
B <- 1000
#we will be sampling 8 columns for our resampling 
m <- 8
#set n to the number of rows in the training set aka data1
n <- nrow(data1)

#note we are only using the column names below to resample as some of the variables have been dropped
col_names <- c("age","workclass","education","marital_status","occupation","relationship","race","sex", "capital_gain","capital_loss","hours_per_week") 

#we want to predict a class for each row of data in the test data aka dat1_test
#to do this we will have to implement transpose and subsetting later on
#GOAL: we basically want to output a transposed data frame of B rows for each row number in the test data
#WHY: having the row numbers of the test numbers as column names will allow us to easily get average probabilities for >50K and <=50K, then from there we can pass our newly estimated probabilities from bootstrapping to get an accuracy rate for bootstrap_rf
bootstrap_rf<-map_dfr(seq_len(B), function(i) {
  col_names <- c("income", sample(col_names, m)) #selects the col
  income_boot <- data1[sample(n, n, replace = TRUE), col_names]
  tree_income_boot <- tree(income ~ ., income_boot)
  predict_tree<-predict(tree_income_boot, data1_test)
  as.data.frame(t(predict_tree))
})

#the formatting of the transpose makes all even indices as probabilities for being in class >50k and all odd indices as probabilities for being in class <=50K
even_indexes<-seq(2,nrow(bootstrap_rf),2) #indices associated with the >50k 
odd_indexes<-seq(1,nrow(bootstrap_rf),2) #indices associated with the<=50k

#we want to extract all the odd and even rows into two separate data frames
more_than50K_estimate<-as.data.frame(map_dbl(bootstrap_rf[even_indexes,], mean))
less_than50K_estimate<-as.data.frame(map_dbl(bootstrap_rf[odd_indexes,], mean))

#we then want to bind the two to create a final data frame that predicts classes for the test data and was acquired from bootstrapping
mean_bootstrap_rf<-cbind(less_than50K_estimate, more_than50K_estimate)

#finally pass this newly created data frame into the accuracy calculator that compares predictions to actual
accuracy_val2<-calc_accuracy_rf(data1_test,prediction_rf)

#then we will get an accuracy rate 
(accuracy_val2[1]+accuracy_val2[3])/(nrow(data1_test))*100
```







