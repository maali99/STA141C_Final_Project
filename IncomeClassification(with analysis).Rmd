---
title: "STA 141C Final Project"
author: "Sukhween Bhullar"
date: "3/19/2020"
output: html_document
---
```{r include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```
### **Introduction**  
**Goal:**   
The goal of the following project is to analyze the Census dataset named “adult.data” to classify the income group, >50K (greater   than $50,000) or <=50K (less than $50,000), of a specific individual. This data will be split into two parts, training and   testing. We intend to implement bootstrapping with the generalized logistic model and random forest on the training data set in   hopes to predict the income groups of our test dataset.   

**Source of Data:**  
The data was acquired from the UC Irvine Machine Learning Repository <https://archive.ics.uci.edu/ml/datasets/Adult>. The data was   extracted from the 1994 Census database. It consists of 15 attributes of which 1 is the response variable income (>50K and <=50K).   

**Questions:**  
+Which model has the better accuracy rate after implementing bootstrap, Logistic Regression or Random Forest?  
+What is the predicted class of an individual given their demographics?  

**Variables:**    

* income (Categorical):   
    + Levels: >50K, <=50K 
    + Ex. If someone falls in the >50K group they earn more than $50,000. If someone falls in the <=50K group thy earn less than $50,000 
* age (continuous):
    + Integer value greater than 0
    + Minimum: 17, Maximum: 90 
* workclass (Categorical):     
    + Variable dropped from Logistic Model  
    + Working class category that the individual belongs to  
    + Levels: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked  
* fnlwgt (Continuous):     
    + Variable dropped from Logistic Model  
    + Variable dropped from Random Forest  
    + The number of people with those specific “credentials”  
    + Ex. 77516 individuals are age 39, belong to State-gov, with a Bachelor’s degree, were never married, etc and resulted in an income_group greater than 50k  
    + Minimum: 13769, Maximum: 1484705  
* education (Categorical):    
    + Variable dropped from Logistic Model  
    + Levels: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool  
* education_num: (Continuous)  
    + Variable dropped from Random Forest  
    + Number associated with the education category  
    + Ex. 13 is equal to Bachelors degree  
    + Minimum: 1, Maximum: 16  
* marital_status: (Categorical)    
    + Variable dropped from Logistic Model  
    + Levels: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, married_spouse-absent, Married-AF-spouse  
* occupation: (Categorical)  
    + Variable dropped from Logistic Model  
    + Levels: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces  
* relationship: (Categorical)  
    + Variable dropped from Logistic Model  
    + Levels: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried  
    + Example: If someone’s marital_status is Married-civ-spouse then the relationship will be either Wife or Husband  
* race: (Categorical)  
    + Levels: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black  
* sex: (Categorical)  
    + Levels: Female, Male  
* capital_gain: (Continuous)  
    + Variable dropped from Logistic Model  
    + Minimum: 0 , Maximum: 99999  
* capital_loss: (Continuous)  
    + Variable dropped from Logistic Model  
    + Minimum: 0, Maximum: 99999  
* hours_per_week: (Continuous)  
    + Minimum: 1, Maximum: 99  
* native_country: (Categorical)  
    + Variable dropped from Logistic Model   
    + Variable dropped from Random Forest  
    + Levels: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.  

### **Exploratory Data Analysis:**  
Before we begin our model analysis, we did some general exploratory data analysis to explore variables of interest. 
```{r, warning=FALSE, message=FALSE}
#read the data and rename the columns
library(readr)
library(ggplot2)
library(tidyverse)
library(readr)
url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
data1 <- read.csv(url1, header = FALSE)

colnames(data1) <- c("age",
                     "workclass",
                     "fnlwgt",
                     "education",
                     "education_num",
                     "marital_status",
                     "occupation",
                     "relationship",
                     "race",
                     "sex",
                     "capital_gain",
                     "capital_loss",
                     "hours_per_week",
                     "native_country",
                     "income")

#take out the rows with the missing data
data1[data1 == " ?"] = NA
data1 = na.omit(data1)
```

Histograms of all the quantitative variables: age, capital_gain, capital_loss, education_num, hours_per_week, and fnlwgt, were used to visualize the general distributions. From these histograms we may consider taking out the variables 'capital_gain' and 'capital_loss'. The variable 'fnlwgt' can also be dropped because these are weights that are used by the Census, it's not exactly something an individual can enter in. More reasoning behind which variables were dropped will be explained later in the 'Methodology and Process' section. 

```{r, warning=FALSE, message=FALSE}
#histograms of the continuous quanitative variables 
par(mfrow=c(3,2))
hist(data1$age, xlab="Age", main="Histogram of Age")
hist(data1$capital_gain, xlab="Capital Gain", main="Histogram of Capital Gain")
hist(data1$capital_loss, xlab="Capital Loss", main="Histogram of Capital Loss")
hist(data1$education_num, xlab="Education Number", main="Histogram of Education Number")
hist(data1$hours_per_week, xlab="Hours Per Week", main="Histogram of Hours Per Week")
hist(data1$fnlwgt, xlab="fnlwgt", main="Histogram of `fnlwgt")
par(mfrow=c(1,1))
```

The following graphs show a distribution of our response variable, income. The first chart compares income distribution between males and females. The second graph compares how income distribution amongst different ages. The second chart shows the distribution of income split across different education_num values (each number equates to an education level, ex. 13 is Bachelor's degree) and races. The third chart shows the distribution of income split across different education_num values (each number equates to an education level, ex. 13 is Bachelor's degree) and races. 

```{r, warning=FALSE, message=FALSE}
library(gridExtra)
attach(data1)  
par(mfrow=c(1,3))
# Plot ratios of income in male group and female group
fig = ggplot(data = data1, mapping = aes(x = sex, fill = income))
fig1 = fig + geom_bar(position = "fill")

# plot  the number of income > 50K and income <=50K grouped by race and education
fig = ggplot(data = data1, mapping = aes(x = education_num, fill = income))
fig2 = fig + geom_bar() + facet_grid(race, scales = "free_y")

# plot the distribution of age groupped by income
fig = ggplot(data = data1, mapping = aes(x = age, fill = income, color = income))
fig3 = fig + geom_density(alpha = 0.3)

hours_income = table(data1[,c('hours_per_week','income')])
hours_income <- as.data.frame(prop.table(hours_income, 1))
hours_income_over50 = hours_income[(nrow(hours_income)/2+1):nrow(hours_income),]
fig = ggplot(data = hours_income_over50, 
             mapping = aes(
             x = hours_per_week, 
             y = Freq,
             group = income))

grid.arrange(fig1,fig3)
fig2
detach(data1)
```

**Visualization Analysis**  
From data visualizations, we find some features of our dataset. First, the percentage of income > 50K in the male group is higher than that in the female group. Second, there are few adults whose incomes are more than 50K in the lower education number groups. On the contrary, in groups with education number greater than 15, the percentage of income > 50K is nearly 100%. We find this feature in all race groups and sex groups. Third, we find that the distribution of age in income < 50K group and income > 50K group are different. In the income < 50K group, the peak of age distribution is below 25, while in the income > 50K, the maximum of the distribution is around 40. Last, the relationship between income and hours per week is not monotonous. Sixty hours per week has the highest percentage of income > 50K.

### **Methodology and Process**
We had to use multiple methods to acquire a model that would fit the structure of our data. We ended up using the following approaches:

* Generalized Logistic Regression
* Random Forest
* Bootstrap
* Customized prediction accuracy calculator 

The very first step was to split the data into training and test data. To do this we simply used the 80/20 rule, in which 80 percent of our original data would be used to train, while the other 20 percent would be used to test. 
```{r, warning=FALSE, message=FALSE}
#split the rows into testing data and training data 
data1_test= data1[24131:nrow(data1),] #testing data
data1= data1[1:24130,] #training data
```

**Generalized Logistic Regression** 
We decided to fit the data using a binomial logistic model because our response variable of interest, income, is binary and our explanatory variables are mixed with some categorical and some continuous. From our data exploration, we can see that the variables capital gain and capital loss consisted of mainly zeros, so it was dropped from our model as it would not provide a substantial explanation for the relationship between subjects and the income group they fall under. We also chose to exclude education because the variable education number will provide the same information in years. After several attempts of randomly sampling the data in preparation of the bootstrapping process, we found that not all levels of certain categorical variables were showing up. For example, the work class variable had a very small amount of subjects that were without pay so this level showed up in some subsamples but not in others. This made estimation difficult since the data frames were not equal. After much deliberation, those variables were cut from the model. 

As you can see, we used logistic regression without bootstrapping and included all the variables, the end result was a very long list of coefficients. 
```{r, warning=FALSE, message=FALSE}
fullmodel <- glm(income ~., family = binomial, data = data1)
paste("The number of rows in the full logistic model:", nrow(summary(fullmodel)$coef))

```

By removing all the categorical variables minus race and keeping all the quanitative variables minus capital_gain, capital_loss and fnlgwt we get a manageable model. 
```{r, warning=FALSE, message=FALSE}
#the full model gives us approximately 90 estimates, so we shifted to a simpler model, taking out variables
#with a lot of levels
model1 <- glm(income ~ age + race + sex + education_num + hours_per_week, family = binomial, data = data1)

coeffs_simple <- model1$coefficients
as.data.frame(coeffs_simple)
```

**Bootstrap (with Generalized Logistic Model)**  
We implemented bootstrap to obtain an estimate of what the true coefficients are. We separated the data into ten subsections, and then for each subsection we bootstrapped a thousand times. For every iteration of bootstrap, we fitted a logistic model and placed the extracted coefficients into a dataframe. The data frame consists of ten lists that contains 1000 samples of each coefficient. Subsequently, we took the quantiles of each coefficient and reduced the ten lists to acquire an overall interval. The confidence interval covered a wide range which was not very informative when trying to estimate which income group a subject was in. We tested both the upper bounds and lower bounds with the same subject. The subject was a 37 year old Black male with ten years of education (some-college) that worked 80 hours a week. From the data, we know that this subject makes over 50,000 a year. Using the lower bound coefficients, we found a 16 percent chance that this subject makes over 50,000. When we used the upper bound coefficients we found approximately 75 percent chance that the subject makes over 50,000 a year. The estimates were vastly different so we found our model impractical. We further explored by carrying out the same method with the mean and median, however these gave us similar results to the confidence intervals. Our model had a low  accuracy of classifying a subject into the right income group.     

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# For every iteration
# randomize data
# find the slopes

suppressMessages(library(tidyverse))
library(furrr)

plan(multiprocess, workers = 4)
B = 10
n = nrow(data1)

m <- 10
groups <- sample(seq_len(m), n, replace = TRUE)
subsamples<-map(seq_len(m), ~write_csv(data1[groups==.,], str_c("part",.,".csv")))

single_boot <- function(i, ind_part){
  subsample = read_csv(str_c("part",ind_part, ".csv"))
  freqs = rmultinom(1, n, rep(1, nrow(subsample)))
  model1 <- glm(as.factor(income) ~ age + race + sex + education_num + hours_per_week, weight = freqs, family = binomial, data = subsample)
  return(as.data.frame(t(as.matrix(model1$coefficients))))
}

```

After getting running the bootstraps we implemented three different ways to get an estimate of the slopes by using:

+ confidence interval
+ median
+ mean  
The confidence intervals give us a lower and upper bound for estimates:
```{r, warning=FALSE, message=FALSE}
#Confidence interval at alpha =.05
ci_list = future_map(seq_len(m), ~{
  map_dfr(seq_len(B), single_boot, ind_part =.) %>%
    map_dfr(~quantile(., c(0.025,0.975))) } )

ci1 <- reduce(ci_list, `+`) / length(ci_list)
ci_coeffs<-as.data.frame(t(ci1))
ci_coeffs <- ci_coeffs %>% 
  rename("Coefficients_LowerBound"=V1, "Coefficients_UpperBound"=V2)
ci_coeffs
```

The medians give the following estimates:
```{r, warning=FALSE, message=FALSE}
#median using the median function
med_list = future_map(seq_len(m), ~{
  map_dfr(seq_len(B), single_boot, ind_part =.) %>%
    map_dfr(~median(.)) } )

med1 <- reduce(med_list, `+`) / length(med_list)
med_coeffs<- as.data.frame(t(med1))
med_coeffs<-med_coeffs %>% 
  rename("Coefficients_Median"=V1)
med_coeffs
```

The means give the following input:
```{r, warning=FALSE, message=FALSE}
#mean using the mean function
mean_list = future_map(seq_len(m), ~{
  map_dfr(seq_len(B), single_boot, ind_part =.) %>%
    map_dfr(~mean(.)) } )

mean1 <- reduce(mean_list, `+`) / length(mean_list)
mean_coeffs<-as.data.frame(t(mean1))
mean_coeffs<-mean_coeffs %>% 
  rename("Coefficients_Mean"=V1)
mean_coeffs
```

**Random Forest**  
After understanding that the Logistic Regression Model was not the best fit for our data we resorted to implementing another model, the classification/regression tree. Because this model can be used for both categorical and quantitative variables, in the form of a classification tree and a regression tree respectively, it would work perfectly with the kind of data we’re using. To begin, we ran the full model, but we sadly ran into an error saying that we could not have variables with more than 32 levels. This led us to drop the variable “native_country” as it had the most levels. We also chose to drop “fnlwgt” because this variable is just an estimation for weights used by the Census and was not applicable to our model. Finally, we removed “education” because we already had a variable called “education_num” that was associated with the education level of an individual, keeping both would’ve been redundant. The next step was to create multiple trees.  

The following is the initial tree classification model used on age, workclass, education, marital_status, occupation, relationship, race, capital_gain, capital_loss, hours_per_week and sex. The next step was to create multiple trees, to create a random forest and also apply bootstrap to it. 
```{r, warning=FALSE, message=FALSE}
#implement the tree function to 
library(tree)
#note we cannot use all the variables because the tree function has a limit to how many levels we can have
#if we add all the variables then we would exceed that limit, so we used most of the variables, but took out a
#a few of them
tree_income <- tree(income ~ 
                      age+
                      workclass+
                      education+
                      marital_status+
                      occupation+
                      relationship+
                      race+
                      capital_gain +
                      capital_loss+
                      hours_per_week+
                      sex, data1)

plot(tree_income, type = "uniform")
text(tree_income, pretty = 1, all = TRUE, cex = 0.7)
```



