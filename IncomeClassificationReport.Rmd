---
title: "STA 141C Final Project"
author: "Eva Chen, Muhammad Ali, Yaosang Zhan, Sukhween Bhullar"
date: "3/19/2020"
output: html_document
---
```{r include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```
### **Introduction**  
**Goal:**   
The goal of the following project is to analyze the Census dataset named “adult.data” to classify the income group, >50K (greater   than $50,000) or <=50K (less than $50,000), of a specific individual. This data will be split into two parts, training and   testing. We intend to implement Bag of Little Bootstraps with the Generalized Logistic Model on the training data in hopes to predict the income groups of our test data. Based on our dataset we want to implement bag of little bootstraps because then we do not have to worry about the underlying assumptions of distributions. Since the distribution of our dataset is unknown we can turn to bootstrapping to calculate estimates. By using bag of little bootstraps we can split our data to decrease the computation time and make our code more efficient. 

**Source of Data:**  
The data was acquired from the UC Irvine Machine Learning Repository <https://archive.ics.uci.edu/ml/datasets/Adult>. The data was   extracted from the 1994 Census database. It consists of 15 attributes of which 1 is the response variable income (>50K and <=50K).   

**Questions:**  
+Does the model accuracy rate change after implementing BLB on the Logistic Regression?  
+What is the predicted class of an individual given their demographics?
+Amongst the different BLB Logistic Regression estimation methods we used, which one is the "best"?

**Variables:**    

* income (Categorical):   
    + Levels: >50K, <=50K 
    + Ex. If someone falls in the >50K group they earn more than $50,000. If someone falls in the <=50K group thy earn less than $50,000 
* age (continuous):
    + Integer value greater than 0
    + Minimum: 17, Maximum: 90 
* workclass (Categorical):     
    + Variable dropped from Logistic Model  
    + Working class category that the individual belongs to  
    + Levels: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked  
* fnlwgt (Continuous):     
    + Variable dropped from Logistic Model  
    + Variable dropped from Random Forest  
    + The number of people with those specific “credentials”  
    + Ex. 77516 individuals are age 39, belong to State-gov, with a Bachelor’s degree, were never married, etc and resulted in an income_group greater than 50k  
    + Minimum: 13769, Maximum: 1484705  
* education (Categorical):    
    + Variable dropped from Logistic Model  
    + Levels: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool  
* education_num: (Continuous)  
    + Variable dropped from Random Forest  
    + Number associated with the education category  
    + Ex. 13 is equal to Bachelors degree  
    + Minimum: 1, Maximum: 16  
* marital_status: (Categorical)    
    + Variable dropped from Logistic Model  
    + Levels: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, married_spouse-absent, Married-AF-spouse  
* occupation: (Categorical)  
    + Variable dropped from Logistic Model  
    + Levels: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces  
* relationship: (Categorical)  
    + Variable dropped from Logistic Model  
    + Levels: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried  
    + Example: If someone’s marital_status is Married-civ-spouse then the relationship will be either Wife or Husband  
* race: (Categorical)  
    + Levels: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black  
* sex: (Categorical)  
    + Levels: Female, Male  
* capital_gain: (Continuous)  
    + Variable dropped from Logistic Model  
    + Minimum: 0 , Maximum: 99999  
* capital_loss: (Continuous)  
    + Variable dropped from Logistic Model  
    + Minimum: 0, Maximum: 99999  
* hours_per_week: (Continuous)  
    + Minimum: 1, Maximum: 99  
* native_country: (Categorical)  
    + Variable dropped from Logistic Model   
    + Variable dropped from Random Forest  
    + Levels: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.  

### **Exploratory Data Analysis:**  
Before we begin our model analysis, we did some general exploratory data analysis to explore variables of interest. 
```{r, warning=FALSE, message=FALSE}
#read the data and rename the columns
library(readr)
library(ggplot2)
library(tidyverse)
library(readr)
library(formattable)

url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
data1 <- read.csv(url1, header = FALSE)

colnames(data1) <- c("age",
                     "workclass",
                     "fnlwgt",
                     "education",
                     "education_num",
                     "marital_status",
                     "occupation",
                     "relationship",
                     "race",
                     "sex",
                     "capital_gain",
                     "capital_loss",
                     "hours_per_week",
                     "native_country",
                     "income")

#take out the rows with the missing data
data1[data1 == " ?"] = NA
data1 = na.omit(data1)
```

Histograms of all the quantitative variables: age, capital_gain, capital_loss, education_num, hours_per_week, and fnlwgt, were used to visualize the general distributions. From these histograms we may consider taking out the variables 'capital_gain' and 'capital_loss'. The variable 'fnlwgt' can also be dropped because these are weights that are used by the Census, it's not exactly something an individual can enter in. More reasoning behind which variables were dropped will be explained later in the 'Methodology and Process' section. 

```{r, warning=FALSE, message=FALSE}
#histograms of the continuous quanitative variables 
par(mfrow=c(3,2))
hist(data1$age, xlab="Age", main="Histogram of Age")
hist(data1$capital_gain, xlab="Capital Gain", main="Histogram of Capital Gain")
hist(data1$capital_loss, xlab="Capital Loss", main="Histogram of Capital Loss")
hist(data1$education_num, xlab="Education Number", main="Histogram of Education Number")
hist(data1$hours_per_week, xlab="Hours Per Week", main="Histogram of Hours Per Week")
hist(data1$fnlwgt, xlab="fnlwgt", main="Histogram of `fnlwgt")
par(mfrow=c(1,1))
```

The following graphs show a distribution of our response variable, income. The first chart compares income distribution between males and females. The second graph compares how income distribution amongst different ages. The second chart shows the distribution of income split across different education_num values (each number equates to an education level, ex. 13 is Bachelor's degree) and races. The third chart shows the distribution of income split across different education_num values (each number equates to an education level, ex. 13 is Bachelor's degree) and races. 

```{r, warning=FALSE, message=FALSE}
library(gridExtra)
attach(data1)  
par(mfrow=c(1,3))
# Plot ratios of income in male group and female group
fig = ggplot(data = data1, mapping = aes(x = sex, fill = income))
fig1 = fig + geom_bar(position = "fill")

# plot  the number of income > 50K and income <=50K grouped by race and education
fig = ggplot(data = data1, mapping = aes(x = education_num, fill = income))
fig2 = fig + geom_bar() + facet_grid(race, scales = "free_y")

# plot the distribution of age groupped by income
fig = ggplot(data = data1, mapping = aes(x = age, fill = income, color = income))
fig3 = fig + geom_density(alpha = 0.3)

hours_income = table(data1[,c('hours_per_week','income')])
hours_income <- as.data.frame(prop.table(hours_income, 1))
hours_income_over50 = hours_income[(nrow(hours_income)/2+1):nrow(hours_income),]
fig = ggplot(data = hours_income_over50, 
             mapping = aes(
             x = hours_per_week, 
             y = Freq,
             group = income))

grid.arrange(fig1,fig3)
fig2
detach(data1)
```

**Visualization Analysis**  
From data visualizations, we find some features of our dataset. First, the percentage of income > 50K in the male group is higher than that in the female group. Second, there are few adults whose incomes are more than 50K in the lower education number groups. On the contrary, in groups with education number greater than 15, the percentage of income > 50K is nearly 100%. We find this feature in all race groups and sex groups. Third, we find that the distribution of age in income < 50K group and income > 50K group are different. In the income < 50K group, the peak of age distribution is below 25, while in the income > 50K, the maximum of the distribution is around 40. Last, the relationship between income and hours per week is not monotonous. Sixty hours per week has the highest percentage of income > 50K.

### **Methodology and Process**
We had to use multiple methods to acquire a model that would fit the structure of our data. We ended up using the following approaches:

* Generalized Logistic Regression
* Random Forest
* Bootstrap
* Customized prediction accuracy calculator 

The very first step was to split the data into training and test data. To do this we simply used the 80/20 rule, in which 80 percent of our original data would be used to train, while the other 20 percent would be used to test. 
```{r, warning=FALSE, message=FALSE}
#split the rows into testing data and training data 
data1_test= data1[24131:nrow(data1),] #testing data
data1= data1[1:24130,] #training data
```

**Generalized Logistic Regression** 
We decided to fit the data using a binomial logistic model because our response variable of interest, income, is binary and our explanatory variables are mixed with some categorical and some continuous. From our data exploration, we can see that the variables capital gain and capital loss consisted of mainly zeros, so it was dropped from our model as it would not provide a substantial explanation for the relationship between subjects and the income group they fall under. We also chose to exclude education because the variable education number will provide the same information in years. After several attempts of randomly sampling the data in preparation of the bootstrapping process, we found that not all levels of certain categorical variables were showing up. For example, the work class variable had a very small amount of subjects that were without pay so this level showed up in some subsamples but not in others. This made estimation difficult since the data frames were not equal. After much deliberation, those variables were cut from the model. 

As you can see, we used logistic regression without bootstrapping and included all the variables, the end result was a very long list of coefficients. 
```{r, warning=FALSE, message=FALSE}
fullmodel <- glm(income ~., family = binomial, data = data1)
paste("The number of rows in the full logistic model:", nrow(summary(fullmodel)$coef))

coeffs_full <- fullmodel$coefficients

predicted_full<-predict(fullmodel, data1_test, type="response")

#here we implemented the library caret and the predict functionto predict the classes
#we then extracted the accuracy rate from the confusion matrix 
library(caret)

predict_df_full<-as.data.frame(predicted_full)

for(i in seq_len(nrow(data1_test))){
  if(predict_df_full$predicted[i]>.5){
    predict_df_full$class[i] = " >50K"
  }else{
    predict_df_full$class[i] = " <=50K"
  }
}

cmGLM_full<-confusionMatrix(as.factor(predict_df_full$class), data1_test$income)
glm_rate_full<-cmGLM_full$overall[['Accuracy']]*100 
```

By removing all the categorical variables minus race and keeping all the quanitative variables minus capital_gain, capital_loss and fnlgwt we get a manageable model. 
```{r, warning=FALSE, message=FALSE}
#the full model gives us approximately 90 estimates, so we shifted to a simpler model, taking out variables
#with a lot of levels
model1 <- glm(income ~ age + race + sex + education_num + hours_per_week, family = binomial, data = data1)

coeffs_simple <- model1$coefficients
coeffs_simple<-as.data.frame(coeffs_simple)

predicted_m1<-predict(model1, data1_test, type="response")

#here we implemented the library caret and the predict functionto predict the classes
#we then extracted the accuracy rate from the confusion matrix 
library(caret)

predict_df_m1<-as.data.frame(predicted_m1)

for(i in seq_len(nrow(data1_test))){
  if(predict_df_m1$predicted[i]>.5){
    predict_df_m1$class[i] = " >50K"
  }else{
    predict_df_m1$class[i] = " <=50K"
  }
}

cmGLM_m1<-confusionMatrix(as.factor(predict_df_m1$class), data1_test$income)

#the glm accuracy rate is calculated here
glm_rate_m1<-cmGLM_m1$overall[['Accuracy']]*100 
formattable(coeffs_simple)
```

**Bootstrap (with Generalized Logistic Model)**  
We implemented bootstrap to obtain an estimate of what the true coefficients are. We separated the data into ten subsections, and then for each subsection we bootstrapped a thousand times. For every iteration of bootstrap, we fitted a logistic model and placed the extracted coefficients into a dataframe. The data frame consists of ten lists that contains 1000 samples of each coefficient. Subsequently, we took the quantiles of each coefficient and reduced the ten lists to acquire an overall interval. The confidence interval covered a wide range which was not very informative when trying to estimate which income group a subject was in. We tested both the upper bounds and lower bounds with the same subject. The subject was a 37 year old Black male with ten years of education (some-college) that worked 80 hours a week. From the data, we know that this subject makes over 50,000 a year. Using the lower bound coefficients, we found a 16 percent chance that this subject makes over 50,000. When we used the upper bound coefficients we found approximately 75 percent chance that the subject makes over 50,000 a year. The estimates were vastly different so we found our model impractical. We further explored by carrying out the same method with the mean and median, however these gave us similar results to the confidence intervals. Our model had a low  accuracy of classifying a subject into the right income group.     

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# For every iteration
# randomize data
# find the slopes

suppressMessages(library(tidyverse))
library(furrr)

plan(multiprocess, workers = 4)
B = 10000
n = nrow(data1)

m <- 10
groups <- sample(seq_len(m), n, replace = TRUE)
subsamples<-map(seq_len(m), ~write_csv(data1[groups==.,], str_c("part",.,".csv")))

single_boot <- function(i, ind_part){
  subsample = read_csv(str_c("part",ind_part, ".csv"))
  freqs = rmultinom(1, n, rep(1, nrow(subsample)))
  model1 <- glm(as.factor(income) ~ age + race + sex + education_num + hours_per_week, weight = freqs, family = binomial, data = subsample)
  return(as.data.frame(t(as.matrix(model1$coefficients))))
}

```

After getting running the bootstraps we implemented three different ways to get an estimate of the slopes by using:

+ confidence interval
+ median
+ mean  

The confidence intervals give us a lower and upper bound for beta estimates:
```{r, warning=FALSE, message=FALSE}
#Confidence interval at alpha =.05
ci_list = future_map(seq_len(m), ~{
  map_dfr(seq_len(B), single_boot, ind_part =.) %>%
    map_dfr(~quantile(., c(0.025,0.975))) } )

ci1 <- reduce(ci_list, `+`) / length(ci_list)
ci_coeffs<-as.data.frame(t(ci1))
ci_coeffs <- ci_coeffs %>% 
  rename("Coefficients_LowerBound"=V1, "Coefficients_UpperBound"=V2)
formattable(ci_coeffs)
```

The medians give the following beta estimates:
```{r, warning=FALSE, message=FALSE}
#median using the median function
med_list = future_map(seq_len(m), ~{
  map_dfr(seq_len(B), single_boot, ind_part =.) %>%
    map_dfr(~median(.)) } )

med1 <- reduce(med_list, `+`) / length(med_list)
med_coeffs<- as.data.frame(t(med1))
med_coeffs<-med_coeffs %>% 
  rename("Coefficients_Median"=V1)
formattable(med_coeffs)
```
   
The means give the following beta estimates:
```{r, warning=FALSE, message=FALSE}
#mean using the mean function
mean_list = future_map(seq_len(m), ~{
  map_dfr(seq_len(B), single_boot, ind_part =.) %>%
    map_dfr(~mean(.)) } )

mean1 <- reduce(mean_list, `+`) / length(mean_list)
mean_coeffs<-as.data.frame(t(mean1))
mean_coeffs<-mean_coeffs %>% 
  rename("Coefficients_Mean"=V1)
formattable(mean_coeffs)
```

**Next Steps: Classification Trees and Random Forest**  
After understanding that the Logistic Regression Model may not the best fit for our data we understood that we might need to implement another model, the classification/regression tree. Because this model can be used for both categorical and quantitative variables, in the form of a classification tree and a regression tree respectively, it would work perfectly with the kind of data we’re using. To at least see a general trend, we ran the full model, but we sadly ran into an error saying that we could not have variables with more than 32 levels. This led us to drop the variable “native_country” as it had the most levels. We also chose to drop “fnlwgt” because this variable is just an estimation for weights used by the Census and was not applicable to our model. Finally, we removed “education” because we already had a variable called “education_num” that was associated with the education level of an individual, keeping both would’ve been redundant. The next step was to create multiple trees, so we applied a bootstrap method to randomly select columns to create multiple trees, in other words a Random Forest.  

The following is the initial tree classification model used on age, workclass, education, marital_status, occupation, relationship, race, capital_gain, capital_loss, hours_per_week and sex. The next step was to create multiple trees, to create a random forest. 
```{r, warning=FALSE, message=FALSE}
#implement the tree function to 
library(tree)
#note we cannot use all the variables because the tree function has a limit to how many levels we can have
#if we add all the variables then we would exceed that limit, so we used most of the variables, but took out a
#a few of them
tree_income <- tree(income ~ 
                      age+
                      workclass+
                      education+
                      marital_status+
                      occupation+
                      relationship+
                      race+
                      capital_gain +
                      capital_loss+
                      hours_per_week+
                      sex, data1)

plot(tree_income, type = "uniform")
text(tree_income, pretty = 1, all = TRUE, cex = 0.7)

#Random Forest

#use the library furrr
suppressMessages(library(tidyverse))
library(furrr)
plan(multiprocess, workers = 4)
#set B to 1000
B <- 10000
#we will be sampling 8 columns for our resampling 
m <- 8
#set n to the number of rows in the training set aka data1
n <- nrow(data1)
#note we are only using the column names below to resample as some of the variables have been dropped
col_names <- c("age","workclass","education","marital_status","occupation","relationship","race","sex", "capital_gain","capital_loss","hours_per_week") 
#we want to predict a class for each row of data in the test data aka dat1_test
#to do this we will have to implement transpose and subsetting later on
#GOAL: we basically want to output a transposed data frame of B rows for each row number in the test data
#WHY: having the row numbers of the test numbers as column names will allow us to easily get average probabilities for >50K and <=50K, then from there we can pass our newly estimated probabilities from bootstrapping to get an accuracy rate for bootstrap_rf
predict_vals<-NULL
random_forest<-future_map_dfr(seq_len(B), function(i) {
  col_names <- c("income", sample(col_names, m)) #selects the col
  income_boot <- data1[sample(n, n, replace = TRUE), col_names]
  tree_income_boot <- tree(income ~ ., income_boot)
  predict_tree<-predict(tree_income_boot, data1_test)
  as.data.frame(t(predict_tree))

})

#the formatting of the transpose makes all even indices as probabilities for being in class >50k and all odd indices as probabilities for being in class <=50K
even_indexes<-seq(2,nrow(random_forest),2) #indices associated with the >50k 
odd_indexes<-seq(1,nrow(random_forest),2) #indices associated with the<=50k
#we want to extract all the odd and even rows into two separate data frames
more_than50K_estimate<-as.data.frame(map_dbl(random_forest[even_indexes,], mean))
less_than50K_estimate<-as.data.frame(map_dbl(random_forest[odd_indexes,], mean))
#we then want to bind the two to create a final data frame that predicts classes for the test data and was acquired from bootstrapping
mean_rf<-cbind(less_than50K_estimate, more_than50K_estimate)
```

**Next Steps: Bag of Little Random Forests**  
Unfortunately with the time restrictions we only had enough time to create one BLB model, and were not able to create the Bag of Little Random Forests, but to further our knowledge we wanted to at least attempt the code.  
To create our Bag of Little Random Forest we would use bootstrap and then implement a random forest model. The approach was to resample from each one of the 10 subsamples 1000 times. Then we want to sample eight columns from our final list of eleven columns consisting of age, workclass, education, marital_status, occupation, relationship, race, capital_gain, capital_loss, hours_per_week and sex. In every iteration, we were creating our prediction tree off of our training data and then actually predicting the classification probabilities with our testing data. In the end we wanted to create 1000 different classification probabilities for our testing data, resulting in approximately 6,000,000 rows of data to process for 10 different bootstrapped datasets. To visualize, one can imagine one iteration of a test data set consisting of ~6000 rows, and we create 1000 different probabilities. After getting our 1000 probabilities for each row, we want to calculate the mean probabilities of that row being classified as >50K and <=50K. Then we simply compare which probability is higher and classify each row. This would create a mean prediction value for each one of the subsets, so now we go from ~6,000,000 predictions for 10 subsets to ~6,000 mean predictions for 10 subsets, then we want to create 1 final prediction set consisting of ~6,000 rows. The final dataset will be benchmarked for accuracy against the actual test data values. 

```{r, eval=FALSE, warning=FALSE, message=FALSE}
#use the library furrr

# CODE
# suppressMessages(library(tidyverse))
# library(furrr)

# plan(multiprocess, workers = 4)

# CODE
#set B to 1000
# B <- 2

# CODE
# p<- 10 # the number of subsamples

#we will be sampling 8 columns for our resampling 

# CODE
# m <- 8
#set n to the number of rows in the training set aka data1

# CODE
# n <- nrow(data1)

#note we are only using the column names below to resample as some of the variables have been dropped

# CODE
# col_sample <- c("age","workclass","education","marital_status","occupation","relationship","race","sex", "capital_gain","capital_loss","hours_per_week") 

#we want to predict a class for each row of data in the test data aka dat1_test
#to do this we will have to implement transpose and subsetting later on
#GOAL: we basically want to output a transposed data frame of B rows for each row number in the test data
#WHY: having the row numbers of the test numbers as column names will allow us to easily get average probabilities for >50K and <=50K, then from there we can pass our newly estimated probabilities from bootstrapping to get an accuracy rate for bootstrap_rf

# CODE
# bootstrap_rf<-function(i, ind_part){
#    #selects the col
#   print("hey")
#   col_names <- c("income", sample(col_sample, m)) #selects the col
#   subsample = read_csv(str_c("part",4, ".csv"))
#   income_boot <- subsample[sample(seq.int(subsample), n, replace = TRUE), col_names]
#   #convert variables to factors
#   income_boot <- mutate_if(income_boot, is.character, as.factor)
# 
#   tree_income_boot <- tree(income ~ ., income_boot)
#   predict_tree<-predict(tree_income_boot, data1_test)
#   as.data.frame(t(predict_tree))
# }

# CODE
# future_map(seq_len(p), ~{
#   map_dfr(seq_len(B), bootstrap_rf, ind_part=.)} )

#the formatting of the transpose makes all even indices as probabilities for being in class >50k and all odd indices as probabilities for being in class <=50K

# CODE
# even_indexes<-seq(2,nrow(as.data.frame(t(predict_tree))),2) #indices associated with the >50k
# odd_indexes<-seq(1,nrow(as.data.frame(t(predict_tree))),2) #indices associated with the<=50k

#we want to extract all the odd and even rows into two separate data frames

# CODE
# more_than50K_estimate<-as.data.frame(map_dbl(as.data.frame(t(predict_tree))[even_indexes,], mean))
# less_than50K_estimate<-as.data.frame(map_dbl(as.data.frame(t(predict_tree))[odd_indexes,], mean))

#extra step needed here to construct an aesthetic data frame of the data 

#we then want to bind the two to create a final data frame that predicts classes for the test data and was acquired from bootstrapping

# CODE
# mean_bootstrap_rf<-cbind(less_than50K_estimate, more_than50K_estimate)

#finally pass this newly created data frame into the accuracy calculator that compares predictions to actual

# CODE
# accuracy_val2<-calc_accuracy_rf(data1_test,prediction_rf)

#then we will get an accuracy rate 

# CODE
# (accuracy_val2[1]+accuracy_val2[3])/(nrow(data1_test))*100
```

**Customized Prediction Accuracy Calculator**  
Our team wanted to calculate how accurate our models were, but this proved to be difficult because we were trying to implement bootstrapping throughout this project. In the case in which we don’t use bootstrapping and simply use the glm or tree function once without repetition, we could easily use the confusionMatrix function to get the accuracy of our results by testing the respective models on our testing data. With bootstrapping, we would need to create a method that deals with the complexity of our data structure that have gone through transformations. Through a series of simple if else statements we were able to create a function for bootstrapping with the logistic regression model. A similar function to calculate the Random Forest accuracy was created. This allowed us to calculate the accuracy of our models. 

An accuracy caculator returns a vector of the number of:  

+ incorrect values categorized as >50K
+ correct values categorized as >50K
+ incorrect values categorized as <=50K
+ correct values categorized as <=50K

We then use these numbers to calculate the accuracy rate, which is the correct/total. 
```{r, warning=FALSE, message=FALSE}

#accuray calculator for BLB with logistic regression

#initialize counters
incorrect_50plus=0 #counter for number of incorrect classifications of incomes greater than 50k
correct_50plus=0 #counter for number of correct classifications of incomes greater than 50k
incorrect_50less=0 #counter for number of incorrect classifications of incomes less than 50k
correct_50less=0 #counter for number of correct classifications of incomes less than 50k

predict_blb_glm <-function(method, test_data){
  
  #the accuracy vector will contain a vector of all the counters
  acccuracy_vector <- c("Incorrect(>50k)", "Correct(>50k)","Incorrect(<=50k)", "Correct(<=50k)")
  
  #from the test data select the variables of interest
  test_data <- test_data %>% 
    select(age, race, sex, education_num, hours_per_week,income)
  
  #this for loop will predict the income brackets
  for(i in seq_len(nrow(test_data))){
    
    test_val=test_data[i,]
    
    #we must use multiple if else statements to check for the race variable
    #because it's a categorical variable
    #use the odds= Intercept_est+
        #age_beta1*age +
        #`raceAsian-Pac-Islander`_beta *(0 or 1) +
        #raceBlack_beta *(0 or 1) +
        #raceOther_beta *(0 or 1) +
        #raceWhite_beta *( 0 or 1) +
        #education_num_beta *(education_num) +
        #hours_per_week_beta *(hours_per_week)
    
    if(as.character(droplevels(test_val$race))== " White"){

     odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*0 +
        method$raceBlack*0 +
        method$raceOther*0 +
        method$raceWhite*1 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }else if(as.character(droplevels(test_val$race))==" Asian-Pac-Islander"){
      odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*1 +
        method$raceBlack*0 +
        method$raceOther*0 +
        method$raceWhite*0 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }else if(as.character(droplevels(test_val$race))==" Black"){
      
      odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*0 +
        method$raceBlack*1 +
        method$raceOther*0 +
        method$raceWhite*0 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }else if(as.character(droplevels(test_val$race))==" Other"){
      
      odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*0 +
        method$raceBlack*0 +
        method$raceOther*1 +
        method$raceWhite*0 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }else if(as.character(droplevels(test_val$race))==" Amer-Indian-Eskimo"){
      
      odds= method$`(Intercept)`+
        method$age*test_val$age +
        method$`raceAsian-Pac-Islander`*0 +
        method$raceBlack*0 +
        method$raceOther*0 +
        method$raceWhite*0 +
        method$education_num*test_val$education_num +
        method$hours_per_week*test_val$hours_per_week
      
    }
    
    #use the equation exp(odds)/(1+exp(odds)) to get a probability value
    probability = exp(odds)/(1+exp(odds))
    
    # if the probability is greater than .5 then the predicted income is less than 50k 
    if(probability<.5){
      
      pred_income = " <=50K"
      #if the predicted income for the test data is equal to the actual income, increment the correct counter
      if(pred_income == as.character(droplevels(test_val$income))){ 
        
        correct_50less = correct_50less +1
      }else{
      #otherwise increment incorrect counter
        incorrect_50less = incorrect_50less +1
      }
    
    #otherwise the predicted income is greater than 50k  
    }else{
      
      pred_income = " >50K"
      #if the predicted income for the test data is equal to the actual income, increment the correct counter
      if(pred_income == as.character(droplevels(test_val$income))){
        correct_50plus = correct_50plus +1
      }else{
        #otherwise increment incorrect counter
        incorrect_50plus = incorrect_50plus +1
      }
      
    }
    
    
    accuracy_vector<- c( correct_50plus, incorrect_50plus, correct_50less, incorrect_50less)
    
  }
  
  return(accuracy_vector)
}
```

```{r, warning=FALSE, message=FALSE}
incorrect_50plus2=0 #counter for number of incorrect classifications of incomes greater than 50k
correct_50plus2=0 #counter for number of correct classifications of incomes greater than 50k
incorrect_50less2=0 #counter for number of incorrect classifications of incomes less than 50k
correct_50less2=0 

calc_accuracy_rf<-function(test_data, predicted){

  for(i in seq_len(nrow(predicted))){ #if the probability is higher for >50K then it means that predicted income is >50K
    if(predicted[i,1]<predicted[i,2]){
      
      pred_income = " >50K"
    #if the predicted income is equal to the actual income at that level then increment correct
      if(pred_income == as.character(droplevels(test_data[i,]$income))){
        correct_50plus2= correct_50plus2 +1
      }
      else{
    #otherwise increment incorrect
        incorrect_50plus2 = incorrect_50plus2 + 1
      }
    }else{ #if the probability is less for >50k then it means the predicted is <=50K
      
      pred_income = " <=50K"
    #if the predicted is equal to the actual value of test data at that row 
      if(pred_income == as.character(droplevels(test_data[i,]$income))){
    #then increment the correct counter
        correct_50less2= correct_50less2+1
      }else{
    #otherwise increment incorrect
        incorrect_50less2 = incorrect_50less2 +1
      }
    }
  }
  return(c(correct_50plus2, incorrect_50plus2, correct_50less2, incorrect_50less2))
}
```

### **Analysis and Improvements**  
For our final results, we got the coefficients from the Logistic Regression (these coefficients include implementations with and without Bag of Little Bootstraps). The results have been formatted as a table in the following order: Simple Logistic Regression without Bootstrap, then BLBGLM with Mean, BLBGLM with Median, BLBGLM with Confidence Intervals.    
```{r, warning=FALSE, message=FALSE}
coeffs_df<-as.data.frame(cbind(coeffs_simple, mean_coeffs, med_coeffs, ci_coeffs))
coeffs_df <- coeffs_df %>% 
  rename(Coefficients_Simple=coeffs_simple)
formattable(coeffs_df)
```

After implementing the accuracy calculator we created a table of accuracy rates (in %) for the mean, median, and confidence intervals for the Bag of Little Bootstraps with the General Logistic Regression Model and joined it with the accuracy rates for the simpler models that don't use Bag of Little Bootstraps. The table input for accuracy is based on the following models: GLM on the full model, GLM without bootstrap (selected), then BLBGLM with mean, BLBGLM with median, BLBGLM with confidence intervals.      
```{r, warning=FALSE, message=FALSE}
library(formattable)
#use mean1 to calculate an accuracy vector on the test data
accuracy_mean<- predict_blb_glm(mean1, data1_test)

mean_rate<- (accuracy_mean[1]+accuracy_mean[3])/(nrow(data1_test))*100

#use med1 to calculate an accuracy vector on the test data
accuracy_median<- predict_blb_glm(med1, data1_test)

median_rate<- (accuracy_median[1]+accuracy_median[3])/(nrow(data1_test))*100

#use the confidence interval to calculate an accuracy vector on the test data
accuracy_ci1<- predict_blb_glm(ci1[1,],data1_test) #this calculates for the lower bound
accuracy_ci2<- predict_blb_glm(ci1[2,],data1_test) #this calculates for the upper bound

ci1_rate<- (accuracy_ci1[1]+accuracy_ci1[3])/(nrow(data1_test))*100

ci2_rate<- (accuracy_ci2[1]+accuracy_ci2[3])/(nrow(data1_test))*100

accuracy_tree<- calc_accuracy_rf(data1_test,predict(tree_income, data1_test))
tree_rate <- (accuracy_tree[1]+accuracy_tree[3])/(nrow(data1_test))*100

accuracy_val2<-calc_accuracy_rf(data1_test,mean_rf)
#then we will get an accuracy rate 
rf_rate<-(accuracy_val2[1]+accuracy_val2[3])/(nrow(data1_test))*100

accuracy_rates<-as.data.frame(t(c(glm_rate_full, mean_rate, median_rate, ci1_rate, ci2_rate, glm_rate_m1, tree_rate, rf_rate))) %>% 
  rename("GLM (full, no bootstrap)"=V1,
         "Model 1, no bootsrap"=V2,
          "Mean Rate"=V3, 
         "Median Rate"=V4, 
         "CI Rate (Lower)"=V5,
         "CI Rate (Upper)"=V6,
         "Tree Rate" = V7,
         "Random Forest Rate"= V8)

formattable(accuracy_rates)
```

From the above table it is evident that the most accurate model is the General Logistic Regression model that did not use any bootstrapping as it had the highest accuracy rate at 84.58%. On the contrary the GLM we used regressing against the variables that would eventually be used in the BLB are has a lower accuracy rate at 75.96%. We would expect this because the first model uses all the variables, implying that the smaller model may have dropped some significant variables. Given the circumstances of our approach, we had to drop some variables to make this BLBGLM possible, even though a BLBGLM using all variables would be amazing it is not practical in the sense that it creates 96 different estimates. So to move forward we applied our BLBGLM to this logistic model that had less variables. Thankfully, the numbers showed that BLBGLM was effective since the accuracy rates went up for all estimation methods, minus the median estimates. The best results were acquired by using the confidence intervals, in which we acquired much better results.         

As stated before, we were not able to create a Bag of Little Random Forests because of the time crunch, but based on our accuracy rates for the simple classification tree and the Random Forest, we would expect to get even better results with the Bag of Little Random Forests. When compared to the BLBGLM we get better results from Random Forest. This implies that there might be more power to the Random Forest approach given that the accuracy is higher even though the sampling was lower for Random Forest. For the future, we now know that in the case of data with a lot of categorical variables and levels Bag of Little Random Forests would be an effective approach.       

For now we can conclude that the best method to reduce our estimates, in terms of the Bag of Little Boostraps with Logistic Regression, is the confidence interval approach (in the above table under "CI Rate (Upper)" and "CI Rate (Lower)"). So in the case of obtaining an estimate for our classfication, we can use the following estimates to get a lower bound estimate and an upper bound estimate:     

Income = 


